##### Topics
1. Full text search
2. Inverse Index
3. ElasticSeatrch

#### 1. Full text search
Amazon database has Reviews Table

Reviews

| reviewId | productId | userId | ratingOfFive | content                      |
| -------- | --------- | ------ | ------------ | ---------------------------- |
|          |           |        |              | "The product is of quality!" |
As you can see the content is the actual comment in the review in the form of  a text. Now arise a necessity to search for those reviews that contain certain words, e.g. 'product' and 'quality'

In SQL, the query would be like, 

SELECT * FROM Reviews WHERE content LIKE %product% and content LIKE %quality%;

Time complexity - O(N) where N is the total number of entries in the table. Even if we index the table on content, userId or any, this particular search will not be improved. 

NOTE : If the search was using %product or product % i.e. words being a prefix or suffix, then indexing on the content would increase this particular queries, i.e. index(content) or index(reverse(content))

**Full Text Search** : A technique using which the entire text is searched for specific data like keywords.

SQL databases are not optimal in such cases. Similarly none of other databases like document, key-value, col, graph, vector etc. also do not specifically provide any optimal method of full text search.

This is not a matter of which database to use, but which datastructure.

#### 2. Inverse Index
Imagine a book. The book has index. Index maps the page number to the topic. In the end of the book, there's a glossary. The glossary will have words against a list of page numbers where the words appear, e.g. flamingo - 1, 3, 18, 25 ... Here glossary acts like a inverse index.

With such a data structure, we can easily identify the entries containing these words.

##### Building an inverse index
Q. Which database should be used?
The very first idea would be to use key value, where key is the word and the value is a list of row id where the word appears. But this is inefficient due to a number of problems.

- Problems with Natural language.
Natural language has issues like
- presence of many stop words like the, on, in , of etc. that are not important for searches or text analysis and whose presence can make the process inefficient. Almost 8% of all sentences is 'the'.
- Synonyms - multiple words having same meaning.
- Words with different meaning e.g. lead

Natural language processing - method using which natural language is processed to be better handled by computers. 

Lets learn with a real word review example
>I bought this for my husband who plays the piano. He is having a wonderful time playing these old hymns. The music is at times hard to read because we think the book was published for singing from more than playing from. Great purchase though!

It can involve multiple processes like, 
- Tokenization - Split the text into tokens or words
- Stop word removal - Remove all stop words/filler words or bad words. 
	- From the above example, words, like I, for, my, who, the, is, a, at, to, from... etc. are removed.
- Stemming and Lemmatization - 
	- **Stemming** - Reduce a word into its 'stem' or base form by removing suffixes like 'es', 'er', 'ing', 'ion' etc. Stemming alone can lead to issues because words like car**ing** -> car which is wrong. 'Bought' in the example will not be stemmed. So just removing suffixes is not enough.
	- **Lemmatization** involves a pre-constructing large mapping of different words against the roots. From the above example, 'bought' -> buy, 'plays'->play, 'wonderful'->good, 'time'->time

Now, to solidify lets take the example of storing 4 reviews.
![[Screenshot (6).png]]
##### Handling Queries
Lets say the user has typed in certain words. Do we query all the sentences that has all the words or some of the words, or some sentences having some of the words, or all sentences by relevance?

The most useful scenario is responding with sentences by **relevance**. 
- Relevancy score
A relevance score is a metric that indicates how relevant something is to a query or search. One of the relevancy score is TF-IDF score.

![[Screenshot 2025-02-18 203345 1 1 1.png]]
If we are searching for a word, then a document that contains that word a lot should have more relevance w.r.t the word cat. This is indicated by 'term frequency'. As word count increases, TF increases for that word **in that document**.

A more common word will have less specific information it has. Therefore, a word which is rare should have more relevance than that which is common. This is indicated by 'Inverted document frequency'. As the number of a word across all the documents increases it's IDF decreases.

If the query contains multiple words, then the **sum of** TF-IDF of each word is calculated for each document as the relevancy score.

Now lets dry run an example using the above. Query = "I love kitties"

- Stop word removal - 'I' will be removed.
- Stemming and Lemmatization - 'love cat'

From the inverted index datastructure, we fetch those entries that contains at least one of the word in the processed query. We see 'love' and 'cat' appear in documents 1, 3 and 4. For each of the document, we calculate the TF-IDF.

doc 1 - tf(cat) x 1/df(cat) + tf(love) x 1/df(love) = 2 x 1/3 + 1 x 1/3 = 1 
doc 2 - tf(cat) x 1/df(cat) + tf(love) x 1/df(love) = 1 x 1/3 + 1 x 1/3 = 2/3 = 0.66
doc 3 - tf(cat) x 1/df(cat) + tf(love) x 1/df(love) = 1 x 1/3 + 1 x 1/3 = 2/3 = 0.66

NOTE : Normally we take **log**(1/df) because df are usually very high and 1/df will be negligible causing TF x IDF ~ 0. Log levels out the second term.

So far we have not taken scaling into account. [Apache Lucene](https://lucene.apache.org/) uses something like that. But Amazon has over 1 trillion reviews and people search billions of times/day. Storing in one machine has the problems of:
- Single point of failure
- not able to store trillions of reviews
- not about to handle billions of queries/day

#### 3. Elastic Search
##### Handling Scaling
In order to handle scaling, we need sharding. ElasticSearch is a search engine which provides a large search functionalities in a distributed design.


**Deciding sharding key**
We know sharding key should have the following properties :
- Data and load should be evenly distributed.
- Number of machines traversed should be as low as possible.
- Cardinality of key should be high. (Cardinality means the number of values the key can have. If low the number of machines possible will be low as well.)

Here, there's one more condition:
- Sharding key should be a part of the query.

Here we have two types of data to shard
4. The reviews themselves
5. Inverted index

Lets analyze the queries to decide the query for the above two
- Queries on documents
6. Given id, fetch document
7. Given text find matching documents. (will go to the inverted index) 

- Queries on inverted index
8. Given a word, find its df
9. Given a word, find which documents it appears in.

✅ For text docs, best sharding key = doc id
❌ For inverted index, we might assume the key would be the word itself, but it's wrong

**Why?**
**Attempt 1** : Shard the inverted index by word

![[Screenshot 2025-02-18 221649.png]]

Issue 1 : High latency
In such a design, the inverted index of each word will have to be fetched from their respective machines back to the application server. The TF-IDF of each will then be calculated within the application server, and will be sorted based on these.

Let's see how big the inverted index of  word can be. If there are 1 trillion entries and a word is common such that is occurs in 10% of the entries. 10% of that is 100 billion. If the inverted index is stored as a word against a list of pair of document id , frequency of the word in that doc, each entry in the list can have 8 bytes for doc id and 4 bytes for frequency. That alone takes 1200 billion bytes or 1.2 TB of data. Transferring that between the machine and application server and then processingfor TF-IDF and sorting highly inefficient! 

Issue 2 : Uneven distribution
Even if the words are randomly distributed across the machines, frequency of words in English is not even.

By Zipf's law, frequency of an item is inversely proportional to it's rank, which is true in this case.

![[Pasted image 20250218222646.jpg]]

As a result if the machine has somehow multiple top words, then it'll most probably will always have a higher load.

**Attempt 2** : Shard the inverted index by document id

![[Screenshot 2025-02-18 223005.png]]

Here each shard will keep its own inverted index for the document it has, lets say 1 billion document per shard.
Q. Will the document index for each shards will be different?
Intuitively yes, but since the number of document is very high in each shard, the difference will be negligible.

Q. For a query which shard will be relevant to it?
All of the shards. This leads to fan out reads/write(many machine reads) which is highly inefficient.

But the twist is that, in elastic search we will have to use fan-out reads and it works! In order to understand that, let's see the issues with fan out reads and writes first.

- If there are K machines then each query will get converted into K queries.
- We need to wait for all shards to respond. High latency.
- If one shard fails, then it might lead to a failure/roll/back/retry

   