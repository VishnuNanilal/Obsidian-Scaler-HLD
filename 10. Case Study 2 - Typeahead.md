##### Topics
1. Step by Step approach to  HLD Problems in Interviews
2. Google typeahead interview simulation
	1. functional req
	2. non-functional req
	3. estimation of scale
	4. design the HLD
3. Handling Recency

#### 1. Step by Step approach to HLD Problems
Whenever a project is to be implemented, 
1. **Architect (Senior level engineer)** - Designs the **HLD** for the project. Crucial decisions on Load balancer, Application servers, databases used etc. will be designed. 
2. The designed HLD will be passed to the team consisting of SDE1, SDE2, SDE3 etc. who design the **LLD**.
3. Based on the LLD, the final **code** will be written.

- **Journey of designing the HLD**
1. Gather Requirements
	- functional requirements
	- non-functional requirements

2. Scale of the project
Only after obtaining these two information can the architecture design the HLD.
3. Design the High level

**Interview** (Typically 45 minutes)
In an System design interview (Both LLD and HLD), ask a lot of questions. Lead the interview. The questions are often intentionally vague. 
e.g. Do the HLD of Google typeahead

From the question, we gather the following
1. Functional requirements
2. Non Functional requirements
3. Estimate scale
	- Interviewer will not directly give the scale. In real life, a product manager communicates with the architect the business logic like expected number of users and such. From this architect will have to estimate the scale like the number of requests/second and such.
4. Design along with the details about working of design.
5. Follow-ups - Once the design is complete, the interview will ask any follow-ups which might consists of integrate another functionality or changing the already implemented functionality.
	e.g. We might be given a chat implementation in Whatsapp. Follow-up might be to add a group chat as well.

###### 1. Functional requirements
- features that you should be able to support in your design. 
A system can have many features, e.g. Google typeahead can have personalized suggestions, spelling mistake correction etc. But all the features cannot be implemented in a 45 min interview. Therefore, during functional requirement phase, you and the interviewer should communicate and collaborate to figuring out the MVP. (Minimum Viable Product).

> Minimum viable product - Smallest set of requirement to be able to do a Proof of Concept (POC) of an idea.

###### 2. Non-Functional requirements
- How  those features in a functional requirement should work
- Expectations with respect to the functional requirements

Example, 
- Consistency and Availability for different features. You should give suggestions, rather than asking the interviewer.
- Latency and Consistency when the System doesn't have partitions. PACELC

###### 3. Estimation of scale
How much load the system will be getting.
- No. of request/second, type of requests expected
- How much data the system might need to store

From these two metric, we get to know
- Do I need to do sharding or not?
e.g. If the size of data that need to be stored is 50 GB and no of requests are high. We'll just have one db machine and replicas.
e.g. If the size is 500TB, then we might need to have sharding, sharding+replica, multimaster architecture etc.

- Is the system read heavy write heavy or both.
	- It helps to choose the correct DB.
	e.g. 
	If there's a read heavy system, SQL would be good. 
	If it's a write heavy, noSQL would be good. 
	For both, there's no single DB that can do both. So we might need to have multiple types of DBs or **try to tame down one type of query**. By taming down one type of query, we make a read and write heavy system to read or write heavy and then it can be treated as that. e.g. If there's a read and write heavy system, we can implement caching to reduce read load. Now the system can be treated as a write heavy one.

**Advice** - You'll have to do a lot of 'guesstimates'. 
Guesstimates -  Take an initial number and from that, basis multiple assumptions and come to load req.
e.g. - There's a flat sale of Flipkart. We have to estimate the number of requests for that. Let's say the Ad was shown to 100 million people. From that 10% was interested i.e. 10million. Of that 50% will actually come to the system i.e. 5 million. So we'll have 5 million/60 requests per second.

**Note**: We can have wrong assumptions and the interviewer wont hold you against wrong assumptions, but is more interested in seeing how we have arrived at the requests/second assumption. 

**Back of envelope calculations** i.e. basic approximate calculations
Lets say we get to know there were 50millions users coming in one day. Therefore (50 x 10^6)/(60 x 24 x 60) per second. This is not accurate, but approximate. Whenever we are approximating, do it on worst-case side.

###### 2.5 List all APIs
Sometimes, the API list will also help with the design. Mentioning these APIs will clarify. e.g. "Are these APIs a good understanding of the requirements? getFeed( ), createFeed( )"
#### 2. Google Typeahead interview simulation
1. Functional requirements / POC / Core features / MVP (**4-5mins**)
Q. Should the typeahead start after typing a certain number of chars or begin as soon as you type something?
A. Lets say the typeahead starts as soon as you type something.

Q. We'll display the suggestions based on popularity of the search. Is this good?
A. Yes, let's go with that.

Q. How many suggestions should be shown? Is 10 suggestions enough?
NOTE: Even this will have an impact on the system as the system is to scale.
A. 10 suggestions sounds good.

Q. Spellcheck?
A. Let's keep it simple and not implement spellcheck. Let's do the suggestion based on prefix. 

Q. Should we show personalized suggestions?
A. No, we can skip that.

Q. Clarification question. Should the popularity based on the entire search count, or recent search count.
A. Lets say it's based on the complete search count. As a follow up we can implement the frequent search later.

- MVP features
- Start as soon as typed
- display based on popularity
- 10 suggestions

2. Non-Functional requirement
- Availability and consistency
In a typeahead, we prefer **availability** over consistency.
We have to build a highly available system

- Latency and consistency
In a typeahead, we should give priority to low latency over consistency. Obviously we'd need suggestions quicker than the person types the entire query.  

3. Estimate of Scale
We know estimate of scale is on Storage and queries per second

**storage**
 In order to show suggestions, we need to have to store the frequency of every query in the past. Let's estimate the storage. 

We mostly need a starting number of users/day which is allowed to be asked from the interviewer. 

Let's say there are 500 M users of Google per day. On average a user makes 20 queries. 
Total searches = 20 x 500M = 10 B queries /day
All the 10B will not be new queries. Lets say there are about 20% of these are unique.
Therefore, # of unique queries = 20% of 10B = 2 B
Each query can be 40 char in length on average
Therefore, 2B x 40 char = 2B x 40 Bytes = 80 Billion Bytes = 80 GB of queries/day

Let's **provision** our system for 5 years
80 GB x 5 x 365 = 80 x 2000 = 160 000 GB = 160 TB

These are the queries we need to store. We also need to store the frequency for 2B queries.
2B x 8Bytes(frequency) x 5 x 365 = 2 x 8 x 2000 GB = 32 000 GB = 32 TB

Therefore, in total we'd need 192 TB of data for storing frequency of every search for 5 years. 
192 TB is too large for a machine. **Therefore we have to shard the data.**

**Queries per second (QPS)**
For queries per second we need to first think if it has peak times or remains constant throughout the day. 
e.g. A stock market server might have peak queries when the market opens or closes. In case of google, since its global, the queries remain the same (Night time in India is daytime in USA.) 

We have figured that there are 10B queries per day.
Therefore 10 x 10^ 9 / 24 x 60 x 60 ~ 10<sup>5</sup>per second.
~ 0.1 M searches / day
Whenever a search happens we have to update the frequency. 
This is the # of write queries

Assume a user writes 10 character before clicking search. For each character a typeahead query will be send. Therefore 10 x 0.1 M queries = 1 M queries/second
This is the # of read queries

**Read heavy or write heavy or both?**
A system is read or write heavy when one is statistically huge compared to other. Here they are not that different. Therefore the system is both. 
We already know we have to either use multiple db for different roles or we should somehow reduce one type of queries.

4. APIs
```
- List <String> getSuggestion(query)
- void search(query)
```
6. Design
For the time being, lets imagine the system uses only one machine. We'd use a Trie datastructure to store the search frequency.

In this Trie, each node consists of the frequency of searches of that prefirx represented by the Trie as well as a list of top 10 suggestions for that prefix.

```java
Node{
	Long frequency;
	List<String> suggestions;
}
```

For getSuggestions( ) we'd simply traverse through the Trie and return the suggestion List. 
Time complexity - O(length of the prefix)

For setFrequency( ) API which gets called when ever someone makes a search, we'd have to traverse as well as update all the List in all the lists in all the ancestor nodes from current prefix to the root Node. The update is incrementing the frequency of the current search. List is only 10 in length. Traversing and updating it is also O(1). Therefore, this will also only takes O(length of the prefix).

**Now let's go back to a scalable system.** In such a system, we cannot use a Trie as we cannot shard the Trie.

 We know we have to store two data wrt to a string,
 - frequency of that string as a search query
 - top 10 results with that string as prefix

**Using HashMap instead of Tries**

```java
HashMap<String, Integer> 
HashMap<String, List<String>>
```

We can store this in a Key value DB. Redis is not Durable. So lets take DynamoDB. For each frequency key, we can add a `FREQ_` prefix and for Top 10, lets add a `TOP10_` prefix. This is a common method to add multiple types of data within the same Collection. 

| key       | value                             |
| --------- | --------------------------------- |
| FREQ_in   | 100                               |
| TOP10_in  | ["india", "indigo", ...]          |
| FREQ_sar  | 234                               |
| TOP10_sar | ["samurai", "sack", "sanity"....] |

_______

Next step would be to figure out the two things about storing in a DB
- handling the size 
- Figuring out the read write heaviness

**Figuring out the read-write heaviness**
Right now write : read = 1 : 10
Even if we reduce reads by 1/10, the ratio becomes 1:1, still read and write heavy. Therefore the only option left with us is to reduce the writes. If we could reduce the ratio of write to 1/100, then ratio would be .01 : 10 or 1:1000 which is a good figure to make the system read heavy.

Now how do we do that?
We keep another K-V database machine which keeps the frequency. Whenever the frequency of a search query becomes 100, first this machine's fr data is reset to 0, and the 100 count is added to the original K-V db machine's frequency for that search prefix. Then the top 10 list is recalculated. This way, instead of all write queries reaching the top10 list, we only change it 1/100th of the original.
The secondary db can be designed as write heavy and optimized for that, and the original db as read heavy.

**Problem with this approach**
Now there are two different databases, read heavy and write heavy. This means more effort, cost etc. to maintain them. Engineers need to know both read heavy and write heavy sides etc.

- **Random sampling**
Another approach would be to use relative frequency. To calculate the most searched terms, we don't need to keep the exact frequency, but relative frequency would be enough. Here, we don't update every write query, but randomly ignore x% of them. Due to the randomness, the probability of the order remaining the same is high. 

Thus through random sampling, we can reduce the write queries by ignoring random write queries while still keeping the relative order same and **all in one type of machine**. 

**One thing to note is, random sampling only works when the write queries are very large and also random which is true in case of Google searching.**

![[Screenshot 2025-01-27 175349.png]]

```java
boolean handleWriteQuery(String str){
	int random = Math.ceil(Math.random(0, 100)*100);
	/*we generate a random number and process the write query only if it's 1.
		i.e. 1% probability.
	*/
	if(random==1){
		db.updateFrequency("FREQ_"+str);
		db.updateTop10("FREQ_"+str);
		return true;
	}
	return false;
}

boolean handleReadQuery(String str){
	return db.getTopSearches("TOP10_"+str);
}
```

___
Now, lets go to the next step, **handling the size**
We still have 132TB of data to store. How do we shard and shard by what sharding key?

Sharding key should have,
1. Uniform distribution of load
2. Query to execute on as less DB machine as possible.

We know we do three operations on the database,
- db.getSuggestion(str)
- db.updateFrequency(str)
- db.updateTop10(str)

the updateTop10 query not only updates the list, but also should update the list on all prefixes. In other words we want a search term as well as all it's prefixes in the same machine to make the queries optimal. The shard key we choose should do the same.

1. Shard by first character
This way, a string and its prefixes will be in the same machine.
e.g. india, indi, ind, in, i.
The problem with this approach is that at max we can only have 26 machines.
We have 132 TB/26 ~ 7TB of data in one machine. Size is still to large.

2. Shard by first two character
Here first two characters ("in" in india) forms the same hash key in CH and goes to the same machine, but first character form another hash key and be in another machine.
i.e. india, indi, ind, in will be in one machine, while "i" can be in another machine. Therefore we might have to traverse to two machine for updateFrequency queries of prefixes. But traversing in two machines is not bad.

Also, now we can have 26 x 26 machines.
132TB / 26 x 26 ~ 200GB
This is more optimal.

3. Shard by first 3 character
As we can see, as first characters becomes 3, the number of machines will be 3 and load in each machine will also decrease by 26 x 26 x 26.

**Now, lets simulate someone searching "India" to be really clear**
Types:
- "i" => getSuggestions("i")  // checks one machine
- "in" => getSuggestions("in")  // checks one machine
- "ind" => getSuggestions("ind")  // checks one machine
- "indi" => getSuggestions("indi")  // checks one machine
- "india" => getSuggestions("india") // checks one machine
Presses Enter to search
 - Enter => 
	 - ignores 99% of the time
	 - doesn't ignore 1% of the time
		 - updateFreq("india") //checks one machine
		 - updatesTop10("india") //checks one machine
		 - updatesTop10("indi") //checks one machine
		 - updatesTop10("ind") //checks one machine
		 - updatesTop10("in") //**Might be present in another machine**
		 - updatesTop10("i") //**Might be present in another machine**

___
#### 3. Handling Recency
The implementation so far do not consider recency or trend. Let's say a new movie called "Friendship" became a huge hit and you would ideally want it to be shown as a suggestion. But "Friends" TV show was around for over 20 years and by this implementation it'd have more preference when someone types "fri".

**Solution**
In certain time intervals, we go to the machine and halves the frequency of everything

| Time | FRIENDS original count | new search count | Friendship original count  | new search count |
| ---- | ---------------------- | ---------------- | -------------------------- | ---------------- |
| 1    | 10000                  | 500              | 50                         | 10000            |
| 2    | (1000+500)/2 = 750     | 100              | (50+10000)/2 = 5025        | 20000            |
| 3    | (750+100)/2 = **425**  | 150              | (5025+20000)/2 = **12512** | 15000            |
|      |                        |                  |                            |                  |
As you can see, by time period 3 the trending topic has already come on top at 12512. Similarly if the new trend dies, by the same logic, the older FRIENDS come on top.

___

