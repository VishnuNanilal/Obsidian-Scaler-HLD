##### Topics
1. Step by Step approach to  HLD Problems in Interviews
2. Example - Google typeahead
	1. functional req
	2. non-functional req
	3. estimation of scale
	4. design the HLD
3. Hot to have recency in typeahead suggestion

#### 1. Step by Step approach to HLD Problems
Whenever a project is to be implemented, 
1. **Architect (Senior level engineer)** - Designs the **HLD** for the project. Crucial decisions on Load balancer, Application servers, databases used etc. will be designed. 
2. The designed HLD will be passed to the team consisting of SDE1, SDE2, SDE3 etc. who design the **LLD**.
3. Based on the LLD, the final **code** will be written.

- **Journey of designing the HLD**
1. Gather Requirements
	- functional requirements
	- non-functional requirements

2. Scale of the project
Only after obtaining these two information can the architecture design the HLD.
3. Design the High level

**Interview** (Typically 45 minutes)
In an System design interview (Both LLD and HLD), ask a lot of questions. Lead the interview. The questions are often intentionally vague. 
e.g. Do the HLD of Google typeahead

From the question, we gather the following
1. Functional requirements
2. Non Functional requirements
3. Estimate scale
	- Interviewer will not directly give the scale. In real life, a product manager communicates with the architect the business logic like expected number of users and such. From this architect will have to estimate the scale like the number of requests/second and such.
4. Design along with the details about working of design.
5. Follow-ups - Once the design is complete, the interview will ask any follow-ups which might consists of integrate another functionality or changing the already implemented functionality.
	e.g. We might be given a chat implementation in Whatsapp. Follow-up might be to add a group chat as well.

###### 1. Functional requirements
- features that you should be able to support in your design. 
A system can have many features, e.g. Google typeahead can have personalized suggestions, spelling mistake correction etc. But all the features cannot be implemented in a 45 min interview. Therefore, during functional requirement phase, you and the interviewer should communicate and collaborate to figuring out the MVP. (Minimum Viable Product).

> Minimum viable product - Smallest set of requirement to be able to do a Proof of Concept (POC) of an idea.

###### 2. Non-Functional requirements
- How  those features in a functional requirement should work
- Expectations with respect to the functional requirements

Example, 
- Consistency and Availability for different features. You should give suggestions, rather than asking the interviewer.
- Latency and Consistency when the System doesn't have partitions. PACELC

###### 3. Estimation of scale
How much load the system will be getting.
- No. of request/second, type of requests expected
- How much data the system might need to store

From these two metric, we get to know
- Do I need to do sharding or not?
e.g. If the size of data that need to be stored is 50 GB and no of requests are high. We'll just have one db machine and replicas.
e.g. If the size is 500TB, then we might need to have sharding, sharding+replica, multimaster architecture etc.

- Is the system read heavy write heavy or both.
	- It helps to choose the correct DB.
	e.g. 
	If there's a read heavy system, SQL would be good. 
	If it's a write heavy, noSQL would be good. 
	For both, there's no single DB that can do both. So we might need to have multiple types of DBs or **try to tame down one type of query**. By taming down one type of query, we make a read and write heavy system to read or write heavy and then it can be treated as that. e.g. If there's a read and write heavy system, we can implement caching to reduce read load. Now the system can be treated as a write heavy one.

**Advice** - You'll have to do a lot of 'guesstimates'. 
Guesstimates -  Take an initial number and from that, basis multiple assumptions and come to load req.
e.g. - There's a flat sale of Flipkart. We have to estimate the number of requests for that. Let's say the Ad was shown to 100 million people. From that 10% was interested i.e. 10million. Of that 50% will actually come to the system i.e. 5 million. So we'll have 5 million/60 requests per second.

**Note**: We can have wrong assumptions and the interviewer wont hold you against wrong assumptions, but is more interested in seeing how we have arrived at the requests/second assumption. 

**Back of envelope calculations** i.e. basic approximate calculations
Lets say we get to know there were 50millions users coming in one day. Therefore (50 x 10^6)/(60 x 24 x 60) per second. This is not accurate, but approximate. Whenever we are approximating, do it on worst-case side.

###### 2.5 List all APIs
Sometimes, the API list will also help with the design. Mentioning these APIs will clarify. e.g. "Are these APIs a good understanding of the requirements? getFeed( ), createFeed( )"
#### 2. Google Typeahead interview simulation
1. Functional requirements / POC / Core features / MVP (**4-5mins**)
Q. Should the typeahead start after typing a certain number of chars or begin as soon as you type something?
A. Lets say the typeahead starts as soon as you type something.

Q. We'll display the suggestions based on popularity of the search. Is this good?
A. Yes, let's go with that.

Q. How many suggestions should be shown? Is 10 suggestions enough?
NOTE: Even this will have an impact on the system as the system is to scale.
A. 10 suggestions sounds good.

Q. Spellcheck?
A. Let's keep it simple and not implement spellcheck. Let's do the suggestion based on prefix. 

Q. Should we show personalized suggestions?
A. No, we can skip that.

Q. Clarification question. Should the popularity based on the entire search count, or recent search count.
A. Lets say it's based on the complete search count. As a follow up we can implement the frequent search later.

- MVP features
- Start as soon as typed
- display based on popularity
- 10 suggestions

2. Non-Functional requirement
- Availability and consistency
In a typeahead, we prefer **availability** over consistency.
We have to build a highly available system

- Latency and consistency
In a typeahead, we should give priority to low latency over consistency. Obviously we'd need suggestions quicker than the person types the entire query.  

3. Estimate of Scale
We know estimate of scale is on Storage and queries per second

**storage**
 In order to show suggestions, we need to have to store the frequency of every query in the past. Let's estimate the storage. 

We mostly need a starting number of users/day which is allowed to be asked from the interviewer. 

Let's say there are 500 M users of Google per day. On average a user makes 20 queries. 
Total searches = 20 x 500M = 10 B queries /day
All the 10B will not be new queries. Lets say there are about 20% of these are unique.
Therefore, # of unique queries = 20% of 10B = 2 B
Each query can be 40 char in length on average
Therefore, 2B x 40 char = 2B x 40 Bytes = 80 Billion Bytes = 80 GB of queries/day

Let's **provision** our system for 5 years
80 GB x 5 x 365 = 80 x 2000 = 160 000 GB = 160 TB

These are the queries we need to store. We also need to store the frequency for 2B queries.
2B x 8Bytes(frequency) x 5 x 365 = 2 x 8 x 2000 GB = 32 000 GB = 32 TB

Therefore, in total we'd need 192 TB of data for storing frequency of every search for 5 years. 
192 TB is too large for a machine. **Therefore we have to shard the data.**

**Queries per second (QPS)**
For queries per second we need to first think if it has peak times or remains constant throughout the day. 
e.g. A stock market server might have peak queries when the market opens or closes. In case of google, since its global, the queries remain the same (Night time in India is daytime in USA.) 

We have figured that there are 10B queries per day.
Therefore 10 x 10^ 9 / 24 x 60 x 60 ~ 10<sup>5</sup>per second.
~ 0.1 M searches / day
Whenever a search happens we have to update the frequency. 
This is the # of write queries

Assume a user writes 10 character before clicking search. For each character a typeahead query will be send. Therefore 10 x 0.1 M queries = 1 M queries/second
This is the # of read queries

**Read heavy or write heavy or both?**
A system is read or write heavy when one is statistically huge compared to other. Here they are not that different. Therefore the system is both. 
We already know we have to either use multiple db for different roles or we should somehow reduce one type of queries.

4. APIs
```
- List <String> getSuggestion(query)
- void search(query)
```
6. Design
For the time being, lets imagine the system uses only one machine. We'd use a Trie datastructure to store the search frequency.

In this Trie, each node consists of the frequency of searches of that prefirx represented by the Trie as well as a list of top 10 suggestions for that prefix.

```java
Node{
	Long frequency;
	List<String> suggestions;
}
```

For getSuggestions( ) we'd simply traverse through the Trie and return the suggestion List. 
Time complexity - O(length of the prefix)

For setFrequency( ) API which gets called when ever someone makes a search, we'd have to traverse as well as update all the List in all the lists in all the ancestor nodes from current prefix to the root Node. The update is incrementing the frequency of the current search. List is only 10 in length. Traversing and updating it is also O(1). Therefore, this will also only takes O(length of the prefix).

