##### Topics
1. Distributed System
2. Load Balancing 
	1. heartbeat/health check
	2. round robin/weighted round robin
3. Sharding
	1. Unoptimized Techniques of sharding
	2. Hashing 
	3. Consistent Hashing

#### 1. Distributed Systems
A **distributed system** is a system that consists of multiple independent entities (computers, servers, nodes, etc.) that work together to achieve a common goal. These entities are connected over a network and communicate with each other to share resources, process data, and provide services. 
###### Key Characteristics of a Distributed System:
1. **Concurrency**: Multiple tasks can be executed simultaneously across different nodes.
2. **Scalability**: The system can grow by adding more nodes to meet increased demand.
3. **Fault Tolerance**: The system can continue functioning even if some components fail.
4. **Consistency**: Ensures that all nodes in the system have a consistent view of the data.
5. **Independence**: Nodes can operate and make decisions independently, but they coordinate with others when necessary.

##### Vertical and Horizontal Scaling
Scalability of our system can be handled through vertical and horizontal scaling

**Vertical Scaling** - When the system is scaled by upgrading the existing servers
Pros - 
1. Easier to manage as there's only one machine
Cons -
1. Downtime while application updating
2. Single point of failure
3. Not possible to scale infinitely
4. Cost increases exponentially with scaling as machines with higher specs are less common
5. Not possible to upscale and downscale dynamically based on demand and requirement

**Horizontal Scaling** - When the system is scaled by adding more servers
Pros -
1. Less chances of downtime while updating due to more machines. When application updation is needed, one machine will be updated at a time. This is called **Rolling deploy**. Another advantage of rolling deploy is, even if the updation has a bug, the whole system wouldn't go down. 
2. No SPOF
3. Scaling is possible infinitely
4. Cost increases linearly
5. Dynamically downscale and upscale by only running those machines that are needed

Cons - 
1. Maintenance due to more number of systems 
#### 2. Load Balancer
In a distributed system, a load balancer is a separate machine which forwards the request to one of the application servers. Ideally it tries to forward the request in such a way that the load is equally distributed. 

In such systems, the load balancer will have a public static IP address and the DNS maps the domain name to this IP address. Any requests to the website will be routed to the load balancer. Other application servers need not have a public IP address anymore.
###### What if the load balancer becomes a single point of failure?
Typically, there will be multiple active load balancers running. As soon as one LB dies, one of the active ones start running on the same IP address as the previous.

###### Working of a Load Balancer
A Load Balancer has mainly three purposes
1. Know when a server is up and running to receive requests.
2. Forward requests to running servers
3. Know when a server is down.

###### How will the LB know when a server is up?
An application server will have the IP address of the LB. When its ready to run, it notifies the LB that its running. 
###### How will the LB know which server to forward a request to?
The first idea would be to forward to the most lightweight server. This would require the LB to continuously check the RAM usage and such of all the server whenever a request is to be forwarded. This makes the LB inefficient and complex.

- Round robing algorithm
	In this design, each request will be sent to the server next to the server that handled the last request. After the last server, the next server will be the one with index 0.

Using Round Robin Algorithm, all servers will have the similar load assuming all application server has the same specs. What if each server has different specs? In such cases we use a **weighted round robin algorithm** where those with higher specs will handle multiple requests before moving on to the next server.
###### How will the LB knows when a server is down?
LB cannot rely on the server to tell that its about to die. 
- Health check - The load balancer checks whether a server is running during regular intervals. If the server is not responding, the LB marks it as down.
- Heart beats - The application server keeps pinging the LB that it is alive. If the LB doesn't get a heartbeat for a certain time, the LB will assume the server is down.

#### 3. Sharding
Similar to scaling of servers, the database also will come to point where scaling will be necessary. We opt for horizontal scaling due to the cons associated with vertical scaling here as well. That is, when database machine gets full, we add a new one to it. 

###### How do we decide to which db do we add a data?
1. Divide based on user id, .i.e. first 100 in one db, next 100 in the next db. The problem with this approach is, older users tend to have larger data. So 1-100 will have larger data than 101-200. We are not distributing the data evenly.
2. Region wise distribution. India's data in one machine, USA in another. Here, the db in a popular region will have larger data. Again, even distribution fails.
3. Randomly putting data among the machines. In this design, the data of same user might be distributed across multiple machines. This makes the fetching of data slow as multiple machines might need to be queried.

###### What is Sharding?
Sharding is a database partitioning technique where data is horizontally divided into smaller, more manageable chunks, called **shards**, which are distributed across multiple servers. This improves performance, scalability, and availability of the database, especially in systems with large amounts of data or high traffic.

If we ensure all data of one user is in one machine, querying becomes faster.
Therefore, **Sharding data based on the user which the data belongs to is the standard sharding method.**
###### Sharding Techniques
1. Data should be uniformly sized
2. Data should be fast to query
###### Sharding logic/algorithm
A **sharding algorithm** determines how to distribute data across shards (servers) using the shard key. 
###### Sharding key
The parameter based on which the the sharding algorithm carries out sharding of data eg: bookmarkId, userId etc. We have already seen that it's efficient to store data of a user in one machine. Therefore, the parameter that we normally use is the userId.

shardingAlgorithm(userID) => returns the id of the server/shard where the given data should be stored.

###### **Hashing**
Hashing is a technique where the sharding key is hashed into a hash value using a hash function. This ensures even distribution of data across multiple db machines.

- Modulo hashing - Simple modulo operation as hash function.
	- `hash(101) % 4 = 1` → Shard 1.
	- `hash(202) % 4 = 2` → Shard 2.
	- `hash(303) % 4 = 3` → Shard 3.

The problem with this approach is that whenever a new shard is added or removed, this will lead to redistribution of data across all the machines. This makes the system inefficient.

![[Screenshot (349).png]]

###### **Consistent Hashing**
**Consistent Hashing** is a hashing technique used in distributed systems to distribute data (keys) across multiple servers (nodes) in a way that minimizes data movement when nodes are added or removed.

In consistent hashing, the hash space is treated as a circular ring ranging from 0 to n. There are two similar hash functions, one for hashing server ids and the other for sharding key of the data. The hash value falls between this range. Well designed hash function will help in even distribution of data.

Servers are placed on the ring based on the hash values. Each data will be first hashed, and then stored on the shard closest to it when moving in a clock-wise direction.

**Problem**
- Not everyone's work is reduced evenly with the addition of a new server
- When a db dies, there are chances of **cascading failures**.

###### **Modified Consistent Hashing**
Modified consistent hashing handles both problems associated with consistent hashing. Here instead of having one, we have multiple hash functions for servers and one hash function for sharding key. vEach server will have multiple locations on the hash space.  

![[Pasted image 20250116130646.png]]

- Addition of a new server occupies multiple areas reducing partials loads from multiple servers rather than from one server.
- Death of a server leads to distribution of data across multiple servers rather than a big chunk of data to one server, thus avoiding cascading failures.


