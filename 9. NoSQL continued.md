##### Topics
1. Problems with master-slave architecture
2. Multi-master architecture - Cassandra
3. Tunable consistency
	1. set some params using which you can make Cassandra consistent or available

#### 1. Problems with master-slave architecture
We know in distributed system, data might be stored in a multiple sharded system and each shard might have multiple replicas connected to it in a master-slave architecture. to handle both size of data as well as large number of queries. SQL databases do not suppose sharding or replication by default. So where do we store the logic for managing this system? i.e.
- creating a new shard if needed
- create another replica if needed
- given a key, decide which shard has the data
The first idea would be to use the Application server itself. But there are multiple application servers and keeping them all in sync is not ideal.  

The answer it so to use a separate central manager which takes care of these jobs and the application servers will talk to this manager to figure these things out.

NOTE - In case of NoSQL databases, all these functionalities come in default. We don't need to be bothered about it.

  - How a database manager would work
  1. Create new shards
  2. make replicas as needed
  3. consistent hashing work

 Let's take a scenario where the data gets too large for the current shards. The solution is to add a new shard 

We can add a new shard in two ways- 
a. first add the machine to the consistent hash space and then move the data. Moving data takes time. This time is called staging time. 

But once the shard is connected any data with hash value thats supposed to be handled by it will start to be handled by it. So let's say we are trying to fetch a value x. This value is supposed to be in this new shard, but it was not moved yet. So the application server checks this new shard for the data, will not find it and will return data not present. This is data inconsistency.

This method of adding shard first and then move data will cause data inconsistency, but there's high availability.

b. Move the data into the new shard and then bring to the chain

During this staging time, there are both read and write operations on the database.

**Read operations** - can be handled by the old shard itself until staging it done.

**Write operations** - this itself can be of two ways
- write on new one. What if reads are set on old machine. New writes are not present in old machine - inconsistency.
-  write on old one. One the new shard is live, I will have to copy the newly written data to the new shard. Until done, reads can be inconsistent.
- write to both. Writes will become slow.

A new shard is created when the database is overloaded. All these ways will either cause inconsistency or more load on the system. System maybe impacted heavily.

Now lets talk about **deleting a shard**. This happens when the load has reduced, or if the shard itself has died.

**When load has reduced**, we remove a shard. When a shard is removed, the data inside it will have to go to the shards behind it in the hashing space. (Remember we are using modified consistent hashing here so each shard is present in multiple positions in the space).

**Read operations** - 
- read from new shard - the data might not be transferred yet - data inconsistency
- read from both - reads can be heavy

we can see that here also the issues of consistency and availability comes 

Now, shard deletion can happen it two situations
1.  If the machine is deleted due to load reduction, then we don't need that many shards. 
2. **When the machine has died** - Mostly shard deletion happens when the machine has died. As in the diagram, each machine will be distributed in multiple places and each will have replicas.

![[Screenshot 2025-01-24 205323.png]]
In this situation, we'd still want the same number of master shards. We will promote one of the slaves into master. 

Now, it'll take some time to promote one of the slaves to master. Algorithm like election algorithm will be used and every other slaves should also acknowledge the new master. During this time, there will be read and write operations. 

- **read operations** - Reads can still happen on the replicas
- **write operations** - 
	- write to all the slaves. Writes will be slower.
	- keep writes on hold. Here we wont have availability.

Again, the same issues. 
Conclusion - meanwhile Master-slave architecture is very great, it comes with issues while adding and deleting shard.

#### 2. Multi-master architecture
Multi-master architecture says
- remove the need of replicas. The number of machines reduce.
- don't associate a key to only one shard, instead associate it with x shards, i.e. instead of writing a data to master as well as replicas, we write the data to x masters in front of the master which the data hash value matches with. Replicas are not needed this way.
![[Screenshot 2025-01-24 212107.png]]

One thing to note is that, earlier each shard only needed to store its own data. Now each has to store data of x-1 others. For that we have to horizontally scale it, i.e. **increase the total number of shards**. One thing to note is that the total number of shards now = total number of shards previously + no of replicas.

![[Screenshot 2025-01-24 212909.png]]

As in the diagram, each of data in K region goes to D6, D2, D8

Replicas had two jobs
- act as backup
- distribute read load 

Multi-master architecture also takes care of these two purposes.

Lets see how multi-master architecture handles the ADDING and DELETING a shard problem.

- Deletion
A shard was deleted, there will be read and write operations to take care of
**read operations** will be carried out from other masters and will not be affected by deletion.
**write operations** will be taking place in other masters and similarly will not be affection by deletion.

- Addition
A shard was added, 
**read operations** can be taken place from other masters while data transfer occurs in the new shard.
**write operations** can be taken place from other masters as well.

Conclusion - Multi-master algorithm/design is a much better variation to master-slave architecture. Cassandra uses multi-master algorithm.

#### 3. Tunable consistency and availability
In M-S system, in order to build a consistent system, whenever there's a write comes, you have write to the master as well as the replicas.
For an available system, we write just to master and returns the response.

In Multi-master architecture, there's a concept of tunable consistency, where you can tune the amount of consistency or availability you need to have.

In tunable consistency we have
- **X** - Number of master that share a data
- **W**- Number of machines to write to before returning to a write query.
- **R** - Number of machines that need to have **same value** before returning success to a read query.
 
Less value of W and R => More available and less consistent system.

Lets take an example of a system with X=5, where A's value is changed from 1 to 3.

| A   | A   | A   | A   | A   |
| --- | --- | --- | --- | --- |
| 1   | 1   | 1   | 1   | 1   |
- W=1, R=1

| A   | A     | A   | A   | A   |
| --- | ----- | --- | --- | --- |
| 1   | **3** | 1   | 1   | 1   |
Read can happen in any of the other machines with A=1. Data is inconsistent, but highly available.

- W=2, R=2

| A   | A   | A   | A   | A   |
| --- | --- | --- | --- | --- |
| 1   | 3   | 1   | 3   | 1   |
Read can happen from 1st and 3rd machines with A=1. Data is moderately consistent and moderately available.

- W=1, R=4

| A   | A   | A   | A   | A   |
| --- | --- | --- | --- | --- |
| 1   | 3   | 1   | 1   |     |
Read can still happen from the four machines with A=1. Data is somewhat consistent, but less available.

- W=1, R=5

| A   | A   | A   | A   | A   |
| --- | --- | --- | --- | --- |
| 1   | 3   | 1   | 1   | 1   |
Data never will be inconsistent, but availability is very less.

As we can see, as number of W and R increases, the system becomes more consistent and less available.

 - **Cosistency**

>**W+R > X**

As long as this satisfies, our Reads can never be inconsistent. We can further tune W and R to choose whether to have faster reads or faster writes.

- fast reads => Less R and More W
- fast writes => Less W and More R

We just need to make sure W + R > X to have consistency.

- **Availability**
Keep the value of R and W less to have a available system.
