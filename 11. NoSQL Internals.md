
#### Topics
 1. MemTable, SSTable and LSL Trees

 **Prefix**
 Traversing over a SQL database is easy because they are structured. In other words, each column is of fixed type and row size will also be fixed as a sum of sizes of all elements in the row. 

| id(long - 8B) | name (varchar(50) - 20B) | age (short - 2B) |
| ------------- | ------------------------ | ---------------- |
Total row size = 30B
So if we know the starting address of the table as T, then an element in the 2ns index row and 1st index column will be T+(**2** x 30)+(8+50). (Similar to Array). 

Now coming back to NoSQL, each row is of varying size. The String can be of any length, columns can store embedded data and lists. To get to a particular address, the time complexity is O(N) for now.

##### **How to store data in NoSQL Data**
- Brute Force
Create a file to store all data, data.txt. There are two operations in a db, read and write. Every write operation that comes, e.g.("hello", 1) we append as a string within the file.

e.g. "hi", 100, "hello", 102, "world", 103

The problem with this is, what if someone writes an already existing data again? Technically the preexisting data should be updated. Therefore, for each write, we have to traverse and check. Also if the new write is "hello", 100000 then the rest of the data should be moved to the right. 

if exist:
	update value
	 move data to right
else
	add to the end

TC : O(N)

For read operation, we have to traverse if the data exists, if it does fetch it.

TC : O(N)

Both read and write operations takes O(N)

**Optimization 1 :** 
Do not update on writes, we always append it. This way TC : O(1)
For any read, we traverse from back and finds the latest data as logically the most updated should be present after the stale ones. TC : O(N)

Problem : The writes are fast, but:
- reads are still O(N) and as update increases read gets slower, stale data increases
- file size increases unnecessarily.

Lets also try to optimize reads. We use a hashMap in RAM, of key to memory location of values. When write comes we add or update if exists in O(N), and for read we fetch in O(1). This solves the O(N) issue.

**Problems**
HashMaps use RAM and therefore data is not persistent. But this is normal as when the system restarts hashmap can be repopulated from the file. The real issue is with delete operations. Hoe do we handle delete operations?

- Delete only from Map
Although this works, during system restart the map will be repopulated with deleted data as it still exist in file.
- Delete from both map and file
We have to traverse through the file and delete all instances of the data in O(N)

**Tombstone method (Soft Delete)**
When a delete operation of a data happens, we simply delete the key from hashmap and append a key : -1 at the end of the file. -1 here represents a delete operation. This way even when the map gets repopulated in the end if it encounters a -1 it'll remove the data from the map.

The only problems that remains is unnecessary storage space usage in file as redundant data as well as hashmap. Hashmap lives in RAM and might not be enough to store all data in file. Suppose you have a disk of 1TB. Every entity takes 50B to store. Therefore 1 TB / 50 B = 1000 x 10^9 B / 50 B = 20 x 10^9 entities. Now lets imagine each key value in HashMap takes 8B key and 8B value = 16B. To store 20 x 10^9 entities it'd take 320 GB of RAM. 

Our RAM might not be able to store all the key-values.

**Solution for too much storage use**
Too much storage is mainly caused by duplicate data. **We have to remove them**. We know removal causes O(N), so there's a question of when the data gets removed.
1. Every time updation happens, we remove the preexisting data. This will cause write to be O(N) and we go back to our real issue.
2. During restart. This leads to the same issue if the system never or rarely restarts.
3. An automatic removal that runs every x hours. This solution looks good, but implementation is complex. We may have to keep a copy of the file where removal actually takes place and during removal, more insertions can happen in the original file. Later we have to merge these two files etc to account for these new insertions.

**Using hashMap to remove**
HashMaps only keeps one key at a time with no duplicates. One thought would be to add the file to a hashmap and then write it back to the file. This process would happen at regular intervals keeping the file size reduced. But HashMaps live in memory and again, might not be able to contain all the data in the file.

**Keeping the data in multiple files**
Instead of keeping all the data in one file, we can keep the data in smaller files which can be brought into RAMs at a time. Thus data in each file remain unique. But what about data between two files. Only one file can be brought into RAM as well.

**Mergesort method to remove duplicates**
If data in files are sorted, then we can use two pointers and mergesort method to add only unique data.

e.g.  

| file 1              | file 2              |                             |
| ------------------- | ------------------- | --------------------------- |
| ==A== A B C C D D D | ==A== B C C C D E F | **add A, moves p1**         |
| A ==A== B C C D D D | ==A== B C C C D E F | **moves p1**                |
| A A ==B== C C D D D | ==A== B C C C D E F | **adds B, moves p2**        |
| A A ==B== C C D D D | A ==B== C C C D E F | **moves p1**                |
| A A B ==C== C D D D | A ==B== C C C D E F | **moves p2**                |
| A A B ==C== C D D D | A B ==C== C C D E F | **adds C, moves p1**        |
| A A B C ==C== D D D | A B ==C== C C D E F | **moves p1**                |
| A A B C C ==D== D D | A B ==C== C C D E F | **moves p2**                |
| A A B C C ==D== D D | A B C ==C== C D E F | **moves p2**                |
| A A B C C ==D== D D | A B C C ==C== D E F | **moves p2**                |
| A A B C C ==D== D D | A B C C C ==D== E F | **adds D, moves p1**        |
| A A B C C D ==D== D | A B C C C ==D== E F | **moves p1**                |
| A A B C C D D ==D== | A B C C C ==D== E F | **moves p1**                |
| A A B C C D D D     | A B C C C ==D== E F | **p1 out, adds rest of p2** |
|                     |                     |                             |
Based on all these methods, lets look at the proper duplicate removal method

- **Duplicate removal method (Proper way)**
Whenever a write query comes, we add the data to a TreeMap (instead of a file). TreeMap keeps the data sorted compared to HashMaps. The time complexity to add data is O(log N) but here we are writing to RAM so in practice this is much faster than O(1) write to file.

 There's a side job that executes in regular intervals. This has three steps, 
 - Creates a new TreeMap and copies the data from original map to this
 - clears the original map
 - iterates through the copy and puts all the data in the copy map into a new file
The above 3 steps keeps repeating every x time.

**Compaction process**
Compaction process happens whenever there's more than 1 file. It picks two files and merges them using previously mentioned merge sort method. Each file will be sorted and will not have any duplicates. Neither are brought in memory except for the pointers we use to traverse. TC = O(N+M)

Now we are only left with **reads**

Reads are carried out first in TreeMap. If its not found there it'll be carried out from the latest file to the oldest file. Right now, we have a TreeMap and multiple files.  The worst case of reading a key would be trying to read a non existing key in which the TreeMap and the whole files need to be searched. This is very slow. Obviously we cannot use a hashmap due to the size issue.

- Binary Search (Wrong)
The data in the files are sorted, so we might be inclined to use BS. But similarly to the case where we cannot optimally do BS on a Linked List, we cannot do the same here either. The keys' addresses in K-V might be sorted, but each address might not be of the same size
e.g. 
s = k1 address @204 
e = k4 address @404
m = 404+204 / 2 = 302

we might not even have a key with the given address.

- **Read (right method)**
We keep a TreeMap associated to each file (instead of data  within the file). 
Divide the file into multiple blocks with each block having an address of the first key in that block. 
Now we can populate the treemap with the address of each block within the file its associated with. 

Now lets say we are having a read operation of trying to find a key k3 in f1. We go to the treemap associated with the file. We try to find the block just smaller than k3 in O(log N). Once a block is identified, we linear search on it but since the blocks are much smaller than the total files in the DB, linear search is not an issue here.

#### MemTable, SSTable in LSM
- The TreeMap where we write first is called **MemTable**
- The files which stores the data are called **SSTable**
- The whole Tree is called **LSM (log-structured merge) Tree**, which helps with proper read and write on the databases. Most of the database works using LSM Table including Cassandra.

NOTE
One question that can be asked is, since we first writes to the MemTable would the data be lost before being saved to SSTable. The answer is no. In most database there's something called a **Write Ahead Log**. This saves the log of each write query. Even if the database restarted, the WAL will be first read and missing data will be reconstructed.

___
