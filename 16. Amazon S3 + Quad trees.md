##### Topics
1. Amazon s3 and Storing Large Files
	1. Deciding chunk size
	2. HDFS
	3. Upload and download large files
2. Quad Trees

#### 1. Amazon S3 and Storing large files
Amazon S3(simple storage service) is a cloud based storage service for storing large and very large user generated files including images, audio, video, docs, ZIP, video files, logs, textures etc. which rarely change. 

Other examples of similar storage are Google Cloud Storage, Microsoft Azure Storage, Git LFS(large file storage).

Large files can take up to 50TB of storage. Such files cannot be easily stored in SQL or the noSQL dbs we've learned so far.

**Why not SQL and noSQL covered so far?**
- SQL is structured. Every column is of fixed size. This leads to either wastage of space or not enough space. 
- Large files in SQL can lead to data spanning across multiple blocks leading to increased latency.

The NoSQL like doc, column, key-value, graph etc. uses SSTables and LSM Trees which are also not optimized for storing these large files. 

So our task is to build a NoSQL db optimized for:
- **storing, retrieving, processing** large files of 50TB. 
- The storage should be **reliable and durable**.
- **downloading and uploading** these large files should be possible.
- to **perform Analytics** on these files.

**How to store a very large file?**
A 50 TB file will not fit in one server. Of course there are servers with more than 50TB storage, but it's not ideal. We'll have to split the file into chunks across multiple machines.

**What should the chunk size be?**
- RAM size - Even if the server has 1 TB of storage, we also need to take RAM size into account. Typically, the RAM size of a machine is around 1GB.
- Overhead with chunking - If we create N chunks, then while querying a file we also need to keep track of the machine where the chunk of the file is present. Using this we go to the server and access the chunk.

| file             | chunk_id | server_id |
| ---------------- | -------- | --------- |
| monthly_logs.txt | 1        | 3         |
|                  | 2        | 7         |
|                  | 3        | 1         |
|                  | ...      | ...       |
Each row can take, lets say 50b+8b+16b ~ 100b. 
If chunk size is 10Kb, then to store a 50TB files, number of chunks 
	= 50 x 10^12 b / 10 x 10^3 bytes = 5 x 10^9 chunks = 5 Billion chunks.
We have 5 Billion chunks so to store this info with each row taking 100b, we'd need 100b x 5 billion = 500 GB of storage.

**Conclusion**
- Chunk has to fit in a server
- Chunk has to fit in RAM
- Chunk size cannot be small
##### Deciding chunk size
Database providers do a lot of research and analysis and talks to the client and play around to find the optimal chunk size. 
**Typically a chunk size is a few hundred Mb, 128, 256 Mb**

##### Hadoop's Distributed File Storage (HDFS)
HDFS is another large file storage service

HDFS has multiple nodes to store the above
1. Data Node - chunks are stored
2. Name Node - Mapping(metadata) about chunks are stored as shown in the table below.

![[Screenshot 2025-02-19 184842 1.png]]

##### Reliability and Durability
Durability is present as we always write to disk.
Reliability is only achieved through replication.

If Name Node is lost the data itself is not lost, but we lose the mapping. **Therefore both Name and Data nodes need to be replicated.** Name node is arranged in a master-slave architecture. 
##### Consistency
❌ Eventually consistent - With this setup, if some data change happen in the file, stale data will be served at least for some time. Not allowed. 
✅ We need immediate consistency.

For immediate consistency, we set a **quorum**. In distributed systems, a "quorum" refers to the minimum number of nodes that must agree on a read or write operation for it to be considered valid, essentially acting as a majority vote to ensure data consistency. it'll be at least (x+1)/2 forming a majority where x is the total number of machines.

When a data is written, at least two machines will be written first before success. After than asynchronously the same will be written in more machines.

![[Screenshot 2025-02-19 184842z.png]]

**Replication does not happen in data node level, but on chunk level. Each chunk is replicated across multiple data nodes.**
##### Rack aware and Data center aware algorithms for further reliability
In a typical server room, each server will be stacked on top of each other as in the diagram below.

![[modern-server-stack-22528866.webp]]

Each segment is one server and here, a data node. This arrangement is called a **server rack**. In a server room there will be multiple server racks.

![[Pasted image 20250219191507.jpg]]

Each server rack will typically be connected to a **power supply** and a **router**.

**What happens when the power supply goes down?**
It's likely that when the power goes down, all server in the rack goes down. If the replicas of a chunk is stored in the same rack, there's a change to lose the chunk with power supply.

**Therefore, at least one replica of a chunk should be in a different server rack. This is based on a rack aware algorithm.**

Data centers can also go down together. **Therefore, we can extend the further by making sure that at least one replica should be in a different availability zone.**

##### Uploading and Downloading large files
- Chunking and Dechunking the data
We learned that S3 and HDFS are means through which large files are stored. From the client's perspective (client here is app server which requires the large file storage service here as in the diagram below), they don't care if the file is stored in multiple chunks, but rather they want the data to be as one single unit. All this chunking and de-chunking happens in the app server within the HDFS/S3.

![[Screenshot 2025-02-19 193804.png]]

- When the continuous stream of data reaches the App server in HDFS, it first checks with the Name Node to decide where should the next chunk be stored. 
- The Name Node returns two server ids, primary and secondary server ids. 
- Then the app server collects the data into a buffer of chunk size (128 Mb). 
- When the buffer is full, it'll flush the buffer into the Data Nodes.

```
int primary, secondary = getPrimarySecondaryDataNodes(file_id);
byte[] buffer = new byte[128*1024*1024];
int index=0;
while(byte b = getNextByte() != EndOFFile){
	buffer[index++] = b;
	if(index==chunk_size){
		AsyncSaveBufferDataIntoDataNode(buffer, primary, secondary);
		buffer = new byte[128*1024*1024];
		index = 0;
	}
}
if(index>0){ //if there are any half filled buffer.
	primary, secondary = getPrimarySecondaryDataNodes(file_id);
	AsyncSaveBufferDataIntoDataNode(buffer, primary, secondary);
	buffer = new byte[128*1024*1024];
}
```

##### Downloading
- HDFS Application server go to the Name Node and ask for all the metadata for that file. The NameNode responds with the addresses of the DataNodes that store replicas of each chunk of the file.
- The App server then connects to a DataNode that holds the first chunk and begins reading the data, streaming it into a local buffer.
- Once buffer starts receiving data, application server starts streaming it to the client.
- Once the app server has read the entirety of a chunk, it closes the connection to the current DataNode and connects to the next DataNode holding the subsequent chunk and loads this chunk into the buffer.
- Keep repeating this till all the chunks of the files has been streamed.
#### 2. Quad Trees
 _Quadtrees_ are trees used to efficiently store data of points on a two-dimensional space. It's used in different queries like, Nearest Neighbor query where given a location find the nearest something or find all something in a given range.
e.g. Swiggy, GMaps

Lets understand why we need Quad Trees in the first place

- **Attempt 1 - Query inside a range**
Lets take a Restaurant table

| id  | name | address | lat | long |
| --- | ---- | ------- | --- | ---- |
|     |      |         | x1  | y1   |
|     |      |         | x2  | y2   |
 Now if we know the current lat and long, we can do a query like
 SELECT * FROM Restaurants WHERE ((req.lat-lat)(req.lat-lat) + (req.long-long)(req.long-long)) <= RxR 
 LIMIT 10;
 
![[eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL2Rpc3RhbmNlLWZvcm11bGEuanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo4Mjh9LCJ0b0Zvcm1hdCI6ImF2aWYifX0=.jpeg]]

Let's say we have index on (lat, long) and (long, lat). But as soon as the above formula is introduced the above indexes become redundant as we are using two indices on the same query. (Indexing is simply sorting based on a criteria in order to BS). 

- **Attempt 2 - Rectangle**
We need to find something within the given rectangle. Here, query becomes

SELECT * FROM Rectangle WHERE req.lat between (lat+d, lat-d) AND req.lon between (lon+d, lon-d) LIMIT 10;

![[Screenshot 2025-02-20 124821.png]]

Here also indexing becomes redundant for the same problem. 
Both takes Time complexity of O(N).

Conclusion - You cannot perform range queries on two indices simultaneously.

But lets say we are indexing both lat and lon separately and then we do BS on each separately where first all the points falling within lat+d, lat-d and then all the points within lon+d, lon-d are taken.

![[Screenshot 2025-02-20 130126.png]]

Therefore for 
- Circle - 40,000 x 40,000 = 1.6 x 10^9
- Rectangle - 2 x (2 x 40,000) = 1.6 x 10^5
- Actual requirement - 2 x 2

Although Rectangle approach was better, it's still too much compared to the actual requirement.

- **Attempt 3 - Grid (Uniform)**
Imagine the map is divided into fixed sized grids. Each grid will be numbered. If we know the lat and lon of a point, then we can easily get the grid number using grid height and width. 

![[Screenshot 2025-02-20 132841.png]]

If the entries are indexed based on the grid number, then finding area within and around the grid will be O(log(N)). 

**Problem**
What is the optimal size of the grids?
- If the grid size is too small, then we might not be able to find necessary number of restaurants around it.
- If the grid size is too large, query time can increase
- There is no sweet spot as the number of restaurants can vary depending on the place. 

___

Therefore, instead of a fixed grid, we'd need a dynamic one i.e. Quad Tree

B+ Tree - (1d)
Quad Tree (2d)
K-d Tree (Kd)

Quad Tree reacts to change in density automatically.

To begin with we will define the term **λ** where **λ** = the number of something above which the grid will be split. If **λ** = 5 => If the number of restaurants in the current grid > 5, then the grid will be split into 4 parts (hence the name **quad**). We do this recursively until we get a structure like below.

The tree representation marked in green will have many leaf nodes. Each leaf node will represent the final single grid. Now if you know the source lat and lon of the user, then we can traverse over the Tree such that when it enters a subtree, it checks if the user is within it's limits. If not we exit the subtree, else we continue until we get to a leaf node. 

NOTE : Leaf node will always have **λ** number of restaurants or else it'd have been divided.

Time Complexity - Theoretically, the TC can be O(N) if no split happened, but practically it's O(log 4 N).

![[Screenshot 2025-02-20 133308.png]]![[quad-tree.png]]

Space Complexity

![[1.png]]

![[2.png]]

![[3.png]]

![[4.png]]

![[5.png]]

![[6.png]]

___
