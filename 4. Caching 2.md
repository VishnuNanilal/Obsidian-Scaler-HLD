##### Topics
1. Keeping cache and db in sync
	1. Write Through cache
	2. Write Back cache
	3. Write Around cache
2. Cache Evaluation  
3. Case Study: Scaler Platform Submissions
4. Case Study: FB News Feed

#### 1. Keeping cache in sync with DB
##### When will a cache becomes stale?
When the source of truth undergoes updation or deletion.

There are 3 ways in which cache sync happens
1. Write through cache
Whenever there's an update query, the application server updates the cache first and then the original data and then sends back the response. 

- Write queries are slower as updation only happens after cache updation.
- Reads are fast, as cache will always have the latest data.

Write through are used in read heavy systems.

2. Write Back cache
We first update the cache, mark update as successful, then update the database asynchronously

- Write queries will be fast
- DB updation may not happen. DB inconsistency

This is okay in systems where data consistency doesn't matter. e.g. Instagram likes

3. Write Around cache
First database gets updated, and sends response back. Here update queries skip cache. 

Read queries read from cache. Cache is updated through TTL approach, i.e. cache is invalidated after TTL. If a read query find cache invalidated, cache will be updated then.

This approach can cause data inconsistency, i.e. when a stale cache data exists which has not been TTL yet.

----------------------
Therefore writes can be down in three ways. 

**Write speed** 
Write Back is fastest. Only writes the cache. 
Write Around. Only writes on the db.
Write through. Writes both cache and db.

**Negative Effects**
Write Around - Cache can server old data
Write Back - Data loss can happen
Write Through - Takes time for write queries

#### 2. Cache Evaluation
Cache hit - Data present in the cache
Cache hit - Data absent from the cache

Performance of cache can be evaluated using cache hit rate. Ideally you'd want to increase the cache hit rate.

Lets consider 2 situations and calculate the hit rate

**Case 1**
query - 1 2 3 4 1 2 3 4 1 2 3 4
cache size - 3
eviction strategy - FIFO

query    state    cache
1           miss       1
2           miss       1 2
3           miss       1 2 3
4           miss       4 2 3
1           miss       4 1 3
2           miss       4 1 2
3           miss       3 1 2
4           miss       3 4 2

As you can see, the cache hit rate is 0%

**Case 2** (only change is cache size is 4)
query - 1 2 3 4 1 2 3 4 1 2 3 4
cache size - 4
eviction strategy - FIFO

query    state    cache
1           miss       1
2           miss       1 2
3           miss       1 2 3
4           miss       1 2 3 4
1           hit          1 2 3 4
2           hit          1 2 3 4
3           hit          1 2 3 4
4           hit          1 2 3 4

Therefore, the hit rate of a cache depends on factors like query type, cache size, eviction strategy.
#### 3. Case Study: Scaler Platform Submissions (File data accessing)
In Scaler platform problem submission, when submit button is click, a request is sent to the backend to evaluate the code.

For every problem, there are two files, an input and output file. The input file consists of all input that will be fed in to our code, and the result will be matched with the output file data. If both are same, success will be returned, if not a failure.

Let's build this system.

The backend request will be in the form evaluate(problem_id, languageType, code). The problems are stored in database as:

Problem : 
{
	id: String
	title: String
	description: String
	inputFilePath: String
	outputFilePath: String
}

**Where should the input and output files be stored?**
1. CDN
The input and output files are not meant to be send back to the client, rather only the test results are. Therefore there's no need to use a CDN.

2. Database
Databases are meant to store informative data and not files which are large chunks of data.

3. **File Stores**
Dedicated hard disks designed to store files. It's like a file system accessible over internet. 

**Why File System alone is not enough**
Lets assume there are 70,000 people participating in a contest. Reading 1 MB of data across network can take 150ms. Reading 1 MB of data from hard disk can take 20ms. This totals 170ms for every submission. With 70000 people, this makes the system slow.

Therefore file system alone is not scalable.

**How can I make accessing files fast?**
The file fetching happens between application server and file store. Therefore if we can cache the file in the application server, the network call associated with it can be reduced.

**Optimization 1 :**
Cache the file on the application server. During a contest there will be around 10 problems and they can be cached on the application server and with LRU cache eviction, the system seems to be well designed.

**Problem with updation**
What if, during contest, the input file needs to be updated due to some faulty test case? 

In application server cache (local cache), write through and write back are not possible as we don't know which all application server has what all cached. 

Here we are going for the write around cache. The problem setter updates the file on the file store without updating cache. In write around cache, the only way to sync cache and db is by setting a TTL i.e. in the App server, we set a TTL.

TTL of 1 min
Even with 1 min, there can be a data inconsistency for 1 min.
There can't be a lot of test case updations for a contest. There is unnecessary data invalidation every 1 min.

TTL cannot work well.

**Changing File MetaData**
The problem so far was during updation local caching leads to inconsistency or unwanted invalidation. In this method, during file updation, **we change the metadata like fileName within the database** along with file in the FileSystem.  

This way, when the next read query happens, there will be a cache miss and the application server will be forced to refetch the file from the File System. This only happens when file change happens, and will happen whenever file change happens. This way both the problems of inconsistency and unwanted invalidation can be avoided.
