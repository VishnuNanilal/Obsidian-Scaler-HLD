##### Topics
1. Leaderboard system
2. FB News Feed

#### 1. Leaderboard system
Lets discuss how to design a leaderboard system in a contest of 70,000 participants.

- Brute Force Approach - no caching.
	1. Gateway machine gets request for leaderboard
	2. Sends request to Applicatiob server, which sends request to Database 
		SELECT * FROM CONTESTS WHERE CONTESTID=contestId ORDER BY marks DESC
	3. Database sorts the list
	4. returns the sorted list
![[Screenshot 2025-01-20 205339 1.png]]

The time complexity to sort the list is NlogN. Let's say there will be N request for leaderboard every minute. So every minute there will NxNlogN operations by the database machine.

**This system has sorting computation associated with every database read.**

- Using write around cache
	1. Leaderboard need not be totally accurate, i.e. a delay in stale data serving. We can use cache with cache invalidation using TTL. With a TTL of 1 min, stale data will be served at most for 1 min. 
	2. Database sorting will only happen once every minute while cache invalidation and missing happens. It is no longer dependent on the number of users. 
	
Now lets decide local/global cache to use

- Local cache
![[image-4.jpg]]
- Disadvantages
1. Different data for each server
2. Load on DB depends on number of servers. With increase in number of users, number of application server also tend to increase.

- Global cache
The only disadvantage of a global cache is the latency. Here latency is not a big problem either as a slight delay in leaderboard data fetching is not a big deal.

- Using write through and back
The leaderboard will be updated every time someone submits a problem. In Write Through, whenever a update is happening in the database, the cache will also be updated. Since there are many submissions and associated writing, write through and back makes the cache machine inefficient. 

**Conclusion**
- The functionality has computation associated with reading. We need to use caching.
- Cannot use write through and back since there are many write queries during submissions which will make cache machine inefficient. 
- Write around is used with TTL set as data invalidation method as stale data for a certain period is allowed.
- Local cache is not preferred due to dependency on app server and different data cached.
- Global cache is used as some latency can be afforded.

Write through with TTL invalidation method in global cache is the correct approach in this case.

NOTE: Database is not depended on number of users anymore, but application server and cache machine still does. More Application server can be added to handle the users on its side, while on cache side it will be handled in a different way as we'll learn in the next class.

#### 2. Facebook News Feed
When someone opens the FB app, a news feed will have to be displayed.

- Brute force approach - Compute news feed from database
1. High latency
For every news feed request, we make a DB query. The DB might consists of Users, Friends, Posts tables. Each query would first join the Users with Friends to get a list of friends of current user. Then a select query on Posts table is made such that, a post is selected if its posted by any of the friends of the User. Finally, the Post list is sorted by time and returned. 

2. DB will not fit in one machine
FB has 2x10^9 users. Each entry might be 200 bytes long assuming we store only the name of 100 characters i.e. 200 GB of data. Each users may have around 500 friends on average. Friends table might have 500 x 200GB data which is huge for 1 machine. 

**If DB doesn't fit in one machine we use sharding**. For sharding we use a sharding key. We know that sharding by userId is a good approach and lets assume we did. In this situation, a user, friendId of that user and posts of the user from table User, Friends, Posts will be in one machine. 

BUT, the data of the friends can still be in other machine as the friends are sharded by the friend's userId. In other words, data will still be scattered around multiple machines despite sharding by userId. 

**Therefore to get news feed of a user, we'll have to query into multiple shards**. Sharding is not optimal.

- Global cache
Cache the news feed of each user in a global cache. Every time a user sends a news feed request, instead of the previous DB computation approach, we simply fetch from the cache. Lesser latency due to precomputed data.

- Write around cache
New feed fetching from global cache is fine. But new posts can be made, i.e. cached data gets stale. Normally we handle it with cache invalidation using TTL. For news feed we can have a stale data of 1 min. Large delays affects UX. 

The problem is, a user will not open new feed multiple times in a minute. The cache will almost always remain unused.

**Write around is not optimal.**

We'll update the cache not using TTL, but when any of your friends make a post. In other words, if a person makes a post, the cache of all of their friends will be updated. We have a cache update during writing, which leads to write through or write back approach.

- Write through and back cache
Let's say we use write through/back. Normally this is a good solution, but what if a person has 1 million friends. Whenever they make a post, 1 million new feed cache will have to be updated. **This is called a celebrity problem.**

**Write through/back is not optimal.**

Conclusion: Caching entire news feed of each user is not optimal.

- Caching recent posts of all users
We are designing news feed. News feed tend to have recent posts and mostly about 1 week's old posts. Each post has postId(8 B), time (8 B), userId(8 B), content(500 B), image_url(100 B), more or less 1Kb per post.

By Paleto's principle, about 1% of user make posts, while 99% reads. Lets say each make 5 posts per day on worst case. 1% of 5 x 2Billion is 100 M posts per day
1 GB RAM can handle 1M posts. Therfore a 128 GB RAM can handle 128M posts. 

Therefore if we have a 128GB RAM, it can handle all posts posted by all users in one day. So for the past 7 days, we'd need 700M posts. If we have 7 cache machines, each of 128GB RAM, then it can handle all posts posted by all users in the past 7 days.

Across the 7 cache machines, we'd shard the data by userId i.e. all the posts made by a user in the last 7 days will be in one machine.

Therefore,
For each user's news feed request, we get their friends ids and then get the posts from the cache machines (can be done in parallel), combine the fetched posts and return the news feed. 

Conclusion
This approach is more efficient than the brute force DB computation method where fetching was extremely slow, but complex than caching the whole news feed of a user where fetching was fast, but making a new post was complex.

Now even if someone with a large friends request make a post, it will only update in one place.

