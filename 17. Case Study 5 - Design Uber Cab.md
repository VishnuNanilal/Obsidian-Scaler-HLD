##### Topics
 1. Revision on HLD interviews
 2. System Design on Uber

#### 1. Revision on HLD interviews
##### 1. Understanding the problem statement
Reason by analogy - look  at various use cases of existing apps.
- **book a cab from A to B**
- food delivery (Uber eats)
- Old car reselling (Ola used cars)
- Rent a car for a day (Zoom car)
- Package delivery (Swiggy genie)

##### 2. Functional requirements
- Minimum Viable Product (MVP)
We should only concentrate on MVP. 

Just Read on different roles
- CTO in a new startup 
	- Proof of concept
	- Move fast/fail fast
	- Product market fit
	- Raise funding
- CTO at a mature startup/large company
	- Delegating tasks to others
- Architect at a tier 1 company
	- Single project with minimum features
- Jr Architect/SDE3/SDE2
	- Single feature in a single project

But in an interview, you can never build an entire project. In 45 mins, focus on **2 to 3** features only.

	You should never ask the features to the architect. You should suggest features to the interviewer. If the feature is not needed or has missed, the interviewer will communicate.

- Keep features minimal
- Always think of the features from the user's perspective. Might have to think from different types of users (Rider, Driver...)
- Is this feature unique to what's being built? (Auth etc. are not unique. They are less likely to be discussed.)

- Now let's discuss the features based on their importance

| MVP features (features needed to run the app and are affected by scale)            | Advanced features (features you can live without)         | Bad features                 |
| ---------------------------------------------------------------------------------- | --------------------------------------------------------- | ---------------------------- |
| GOOD                                                                               | LATER                                                     | YOU'LL BE REJECTED!          |
|                                                                                    |                                                           |                              |
| User auth (not needed normally.)                                                   | OTP validation                                            | ==Car types== etc. ^^        |
| User should be able to book a ride from A to B.                                    | Cancel request                                            | Driver sign up, verification |
| Once user has requested a ride, we should find **available** drivers near the user | Live location                                             | Payment types                |
| Notify the driver about booking ==**one by one**== ^^                              | Driver, Ride ETA                                          | Discounts and promotion      |
| Driver can accept or reject the booking ==within a certain time.== ^^              | Route planning. (Uber usually delegate it to 3rd parties) | Price comparison             |
| Notify user that the driver was assigned.                                          | Booking history                                           | Cash back, Coupons           |
| Update driver location                                                             |                                                           | Search for place by address  |
| Driver service sends location update frequently                                    |                                                           | Safety, Fraud detection      |
^^ one by one - If multiple drivers were allowed to see the request, we can have locking mechanism to make sure only one accepts it. But irrespective of that, the driver experience will be affected. Therefore only one driver will be notified at a time.

^^ within a certain time - If the current driver doesn't accept in a given time, the request goes to the next driver.

^^ Car Type - Discussing about car types is a bad idea as it has nothing to do with HLD. 
##### 3. APIs
- requestRide(userId, source, destination) -> returns bookingId or failure
- findNearByDrivers(source) -> returns `List<Driver>`
- notify(userId, payload) -> returns success or failure
//userId can be rider or driver, payload would be json containing extra data. If failure retry
- respondToBooking(userId, bookingId, boolean response) -> returns success/failure
//if failure, retry
- updateAvailability(driverId, status) -> returns success/failure. If failure retry
- updateLocation(userId, lat, lon) -> returns success/failure

##### 4. Non-Functional requirements (Design goals)
- PACELC
	- If Partition :
		- Availability or Consistency
	- Else
		- Latency or Consistency

Consistency
1. No Consistency
2. Eventually Consistency
3. Immediately Consistency

No Consistency is rare. It's mostly used in cases when you can afford losing data e.g. Analytics
Here therefore we have to choose between Eventually and Immediately Consistent.

From the APIs it's clear that we need immediate consistency for most. 

| Immediate             | Eventual                                                                                                        |
| --------------------- | --------------------------------------------------------------------------------------------------------------- |
| request ride          | find nearby drivers (some nearby drivers might not be updated, or some nearby drivers can appear further away.) |
| responding to booking |                                                                                                                 |
| update availability   |                                                                                                                 |
| update location       |                                                                                                                 |
|                       |                                                                                                                 |
Notification consistency is not applicable as it's not stored anywhere, but is just a process.
Think what goes wrong without immediate consistency here.
##### 5. Scale Estimation
Uber has
~ 20 M rides/day
~ 4 million drivers
~ 150 million daily active users
~ 10,000 cities

**Steps 1 to 4 will take 15 to 20 mins out of 45 minutes**

##### 6. System Design
Q. How to find nearby drivers?
Just a nearest neighbor query - Quad Tree

In the previous class, we had restaurant which are static.
Here, we have constantly moving drivers. So normal quad tree might not work.

Q. What will be stored in DB?
- booking details / user details => SQL
- driver location history => Column family (lot of writes and the queries will be based on column like the location of driver for the past time period. Pagination might also be needed.)
- current driver location data => Quad Tree

Q. What should be the sharding key?
1. current driver location data => Quad Tree 
The query on drivers will be mostly based on their location. 
- City
Sharding by city is a good, but each city has different sizes. This may lead to uneven distribution but due to security reasons and gov regulations, different data from different cities are stored in different data centers. Given that data is stored in city-specific data centers, it's logical to partition the Quad Tree by `cityId`.

NOTE : Intercity rides are handled by country level centers for the same reason sharded usually by countryId

2.  booking details / user details => SQL
	Sharded by userId. User is allowed to cross cities etc. 
3. driver location history => Column family
	Sharded by userId

Q. Can the quad tree by static?
Density of the drivers keep changing. Therefore we need a non-uniform grid. We need to modify the quad tree as density changes.

In order to update the quad tree we need location updates from the drivers. We have an API for fetching new location of each drivers and it can be used for this purpose. But we have around 5 million drivers, and if we set a API call every 5 seconds, it means there's 1 million API calls per second.

- Optimizations
1. Don't send location update request by time, instead send by location changes by 1 km. (This is done by driver's front end by checking if location has changed using GPS).
2. If driver is not available, don't send update location requests. 
So from 1 million/second we might have reduced it to 1million/minute i.e. 1 million/60 ~ 20000 location update req/s
###### Internal design
Once the user has booked a ride, success will be immediately returned, and will be registered into SQL. The Book Ride servers are not concerned about communicating with the drivers. Instead a ride message will be added to **Kafka messaging queue.** It can then return success right away.  

The driver assignment servers will start assigning drivers from the data within Kafka. But, again, they themselves are not responsible for finding the nearest drivers. It'll be handled by another set of Nearest driver servers. 

The nearest driver server will contain quad tree. Once the drivers data are fetched, it'll be returned to the Driver Assignment server which again will communicate with the Notification servers. Driver Assignment server will also update in SQL that it's sending a notification to a driver.

The notification servers will then communicate with the drivers and riders and push notifications to them.

The driver acceptance servers will receive the acceptance when a drive accepts the ride notification. Once accepted, it'll communicate with the SQL and make changes.

![[Screenshot 2025-02-20 204616.png]]

Meanwhile Driver Assignment keeps checking with SQL to see if the driver acceptance server has set the process as success. If it timeouts or finds it as failed, then Driver assignment server will check with the next driver.

If driver assignment finds the ride as success, it'll communicate that to the notifications server which will notify the rider.

On the other side, drivers will continuously update location to the Location tracking server. It has to store this data in a NoSQL database (Cassandra or Hbase). It'll also be updating the Quad Tree.

We know the Loc update requests ~ 20000/s. Hbase can handle this, but Quad Tree might not be able to. Whenever the Q-T is updated with location updation,
1. We have to update the location
2. Merging and splitting of Quad Tree might happen depending on the number of drivers going up or down the **位** value. This can be expensive.

Further Optimizations
1. Whenever driver sends location to the Location Tracking server, it responds with the current node id within the quad tree. It also responds with the node boundaries (Top Left-Bottom Right coordinates of the grid) to the front end.
2. Now whenever the driver location changes more than the threshold, we check from the front end if the node boundary has been crossed. The front end compares with the current and previous node boundaries and if the boundary was crossed, a flag will be set.
3. If the flag was not set, i.e. location boundary was not crossed, then the change is only stored into Hbase but not the Quad Tree. But if the location boundary was crossed, then the change is stored in Hbase, and the Quad Tree will also register this change.

Q. Why we store the previous location in front end? 
This was the Location Tracking server doesn't need to keep checking the SQL for the previous location of the driver.

##### Difference between updating driver location and Quad restructure
Updating driver location and restructuring quad tree are two processes.
Suppose a driver changes location such that it triggers a quad restructuring. Along with acknowledging this, **we also need to update the driver location.** 

Updating driver location is important to keep track of the drivers for checking nearby drivers.
Restructuring (necessary merging and splitting) quad tree is important to react to the density changes.

But one thing to note is that, merging and splitting need not be immediately consistent. 

**Optimization choices**
1. On quad tree location update only changes driver location -- don't restructure nodes for every driver location change even if number of drivers in the grid gets more than lambda (or less than). Restructuring only happens periodically such as every 1 hour.

 **OR**
 
2. A driver going  back and forth can cause immediately alternating split and merge. Instead set the 位 in a range such that if merge if x<位1 and split if x>位2. (Something similar to resizing an array list based on 25% full and 75% full instead of a single value).

___

