##### Topics
1. Master-slave architecture latency
2. Zookeeper
3. Corner cases of Zookeeper
4. Multiple zookeepers
#### 1. Master-slave architecture latency
We know M-S architecture is mostly used to handle high read queries where the read queries are divided among the slave or replica machines and the writes first go to the master machine. We need to keep track of the master machine to be able to do this properly. 

- **Load balancer**
The first idea would be to use another load balancer in between the app server and the database. The load balancer keeps track of all masters. But this would cause some latency as its another network call and additional cost to maintain an extra powerful machine like a load balancer. 

- **Register master machine within the App servers**
Master machines rarely changes. If we keep track of the master machines within the app servers themselves, we can resolve them without needing an extra load balancer saving us the latency. 

But when a master machine does change, how is this change communicated?

#### 2. Zookeeper
ZooKeeper is a centralized service forÂ **maintaining configuration information, naming, providing distributed synchronization, and providing group services**. All of these kinds of services are used in some form or another by distributed applications.

Zookeeper has two kinds of hashmaps,  permanent and ephemeral(temporary) hashmaps. Everything in zookeeper is stored as a key value, where the keys are paths against values which is the IP address of the machines. 

The permanent entries are normal files which stores data.  
The ephemeral entries are temporary in the sense that, whoever wrote the entry will have to keep sending heartbeat to the zookeeper in order to make sure the entry keeps stored. When the heartbeat stopes, the value will be set back to NULL.

For every key, there are 3 operations available
- Write once - it can be reset only by the one who wrote.
- Read only - for other machines who were not the one who wrote.
- Set watch - subscribes to a key such that if the value changes the subscriber will be notified.

 **Setting master for a cluster**
 Lets say we have a cluster of 3 machines. In the beginning, all of them send a request to the zookeeper to set themself as the master. Among them the zookeeper chooses one and sets the IP address of that machine against the path as  key-value pair.
 e.g. "/cluster1/masterdb" => IP address of the machine

The rest of the requests will automatically rejected and will be read only and set watch while the master will have a write once setup. This setting continues until and unless the master fails to send a heartbeat and the value will be reset to NULL. The set watch of slaves will help them be notified of the changes so they can try to be the master if the original master ever changes.

On the other end we have app servers which needs to know the IP of masters. They communicate with the Zookeeper for that info and once its fetched, the IP address is cached within them. They also set watch to this data so that if the data ever change, the app server will be notified.

**Summary of setting a new master**
Old master dies=>Heartbeat stops=>value will be set to NULL=>Slaves and app servers will be notified=>Slaves try to be the new master=>One of them will be chosen=>App server will re-fetch new info and the new IP will be cached. 

#### 3. Corner cases of Zookeepers
2. Master dies
 When master has died, after a certain amount of no response, a new master will be assigned. During this time, all write requests fail, in other words, the system will be unavailable. (There's no data lose leading to inconsistency, but unavailability). Unavailability for this amount of time is still alright.

Lets say the slaves tried to set themselves as the master and a new master was selected. This would also take some time of unavailability, which is also fine. 

**The real issue is when during this unavailable time, the first master comes back up and new write requests goes to it leading to false positive response and inconsistency.** 

**Fail safe** : In order to prevent this, when the old master comes back up it checks first whether it still can be the master and if another machine has taken over. Only then it will take any further write requests.  

##### Other use cases of zookeeper
- **Keeping slaves aware of master**
In case some write requests come to the slave, it needs to be redirected to the master first. Therefore, slaves need to know who the master is. Here zookeeper can help. Slaves try to be the master first. If it fails, they check which machine is the master and keeps track of it through zookeeper.

- **Keeping app servers aware of slaves**
Read request can go to any of the machines which are alive. When a slave dies the app servers needs to know about it so no read request will be send to it. **Each slave will have a key-value within the zookeeper through the set watch procedure.** All app servers should set watch on all of these entries as well. 

#### 4. Multiple zookeepers
Zookeepers themselves are not a single machine and a single point of failure but a cluster. Typically they'll have a leader and multiple followers similar to master-slave. Whenever a write query happens, it happens in the leader and then the same will have to happen to at least the majority of the machines. (Typically zookeepers will be odd in number.) Similarly when a read query happens ideally it'll be read from the leader. If leader was not available, then at least a majority of the machines will have to give the same data which will be read.

- **w > x/2
- r either to leader or r > x/2**

Typically zookeeper cluster will be 5 in number.

- **Corner case**
- Leader dies
Zookeeper has a very small traffic. Write and read only comes when a data changes. Zookeeper is more designed to handle the heart beat and sending notifications by set watch. But if the leader has been dead for a long time, then we need to change the leader and similar to master-slave, we will select a new leader here.

