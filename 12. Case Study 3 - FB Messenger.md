##### Topics
1. Interview simulation - FB Messenger
2. Idempotency
3. Bloom Filter

#### 1. Interview simulation - FB Messenger
We know, for HLD of any system there are 5 steps
4. Functional requirements
5. Non functional requirements
6. Estimation of scele
7. API gathering
8. Design

#### FB Messenger interview simulation
9. Functional requirements / POC / Core features / MVP (**4-5mins**)
S. Messenger should have the functionality to send and receive messages.

Q. How about we go with only 1:1 chat or should group chat be supported?
A. Let's just go with 1:1 chat for this implementation

NOTE: A difference between FB Messenger and Whatsapp is that Whatsapp do not store messages on servers. That's why when we change mobiles, the data gets lost.

Q. Shall we support storing message history on servers?
A. Yes message storing in servers should be supported.

S. We would have to display recent messages.

Q. Shall we implement multimedia messages or only text messages?
A. We can keep it simple and go with only text messages for now.

Q. online, last online, read-unread?
A. No need for that in this interview.

10. Non-Functional requirements
- Consistency vs Availability
In a messenger app, consistency would mean preserving the order of messages. Deciding between C and A in messenger applications depends on the application. FB Messenger give importance to consistency.
(Slack and such group chat apps might give importance to availability. Reddit GC give importance to consistency which is might it lags and the order gets corrected at times).

- Consistency vs Latency
We'd want to build a system which is consistent with as low latency as possible. But if only one could be chosen, we'd choose consistency e.g. when an image is being sent the texts sent after it wont load until the image is delivered

11. Estimation of scale
 As you know, the starting user number estimation can be enquired from the interviewer.
Q. How many users are estimated to use the system per day?
A. Let's say there will be around 1 Billion users per day.

In estimation of scale, we need to quantify 3 things,
- Size of storage
- Req per second
- Type of requests (Read or Write heavy)

From these we'll figure out 
- Is sharding needed
- Type of requests

Let's say one person sends 50 messages. There will be 50Billion messages per day. Lets say one message has a schema of:

id: long (8b)
senderId: long (8b)
receiverId: long (8b)
time: **long** (8b)
content: String

NOTE: Time is typically stored as **epoch** time which is long. It represents the number of milliseconds since **1 JAN 1970 GMT+**

Lets say on an average each message content will be around 200 characters i.e. 200b. Therefore each message will be 8+8+8+8+200 ~ 250b. Therefore 250 x 50 x 10^9 bytes/day ~ 12500 x 10^9 b/day
Lets say we are provisioning it for 5 years. Therefore 5 x 365 x 12500 x 10^9 bytes ~ 25 x 10^ 12 ~ 25PB

We have estimated the storage needed. We need sharding.

On a one on one chat, we normally sends a message which will be typically read once. So read : write is 1:1 => System is both read and write heavy. (In a group chat, one message written will be read multiple types. It'd be read heavy normally)

12. API gathering
- sendMessage(senderId, receiverId, content)
We are not sending the time from front end because then users might be able to hack the time to change it etc. 

- getConversationList(userId, offset, limit)
Why offset, limit? (**Tracxn interview question**)
Should we send all the conversations of a user in one go? No. Here we'd have to implement pagination. From **offset** we load **limit** number of conversations.

- getMessages(userId, otherId, offset, limit) or getMessages(conversationId, offset, limit)

___
#### 2. Idempotency
 In a request response cycle, when a client sends a request to the server, the server response with an acknowledgement even before the request is processed. This indicates that the request was received. If the request was not received, no acknowledgement will be send back in which case the request will be resent by the client. But some cases the request will be received but the acknowledgement fails to reach the client. In this case the client sends another request leading to two successful request instead of one. In messenger app, this would be registering same message twice in server which is an error.

Therefore we need a way for server to be able to reject a duplicate request. But how do we identify two messages to be the same. We cannot rely on the content for that. Therefore whenever a message is sent from client, we need to include an unique_id. Inorder to generate a unique id we need an input. What can this input be?

At the same time, 
- two people can send the same content => content alone wont do.
- a person can send same message from two devices => sender id alone wont do
- two messages to the same receiver can be send =>receiverId alone wont do

Therefore we need a combination of timestamp, senderId, receiverId, deviceId. This is called Idempotency.

**Idempotency is this way by which we uniquely identify a message to ensure that no duplicate message will be sent to the server.**

___

13. Design
We understood there will be sendMessage(), getConversationList(), getMessage() APIs. These 3 APIs would work on a Messages Table. 
We'd also need to store around 25PB of data. 

We'd need sharding.

- Sharding key
What should be the sharding key?
Sharding key should be based on uniform load distribution and ideally should reduce querying on multiple machines for a request. Timestamp is a bad reason for uneven distribution of load.

**Sharding by conversation id**
All messages of a particular conversation id will be in one machine. Both sendMessage() and getMessages() will only query one machine. BUT, getConversationList() will have to query multiple machines.

**shard by conversation id but keep another database for Conversations sharded by userId**
Create a new database where you have a conversation Table where we store the complete conversation of a user in a list. This database will be sharded by userId.

Conversations

| userId | List of conversation |
| ------ | -------------------- |
| 100    | [conv1, conv2...]    |
**getMessages()** will only need to query one machine.
**sendMessage()** will query the Messages Table database, and will have to update Conversation of the sender and reciever in Conversations Table. As a result we might have to query 3 messages.
**getConversation()** will only need to query one machine

**shard by user Id**
Here all messages either sent or received by a user will be in one machine. 

**getMessages()** will only query on one machine
sendMessages() will query on 2 machines
getConversationList() will query on 1 machine

The only downside is that we may have to keep copies of one message on 2 machines to cater to user 1(sender) and user 2(receiver). Low latency is more important than storage saving.

- **Failure Situation where we writing to two database machines**
As we've seen sendMessage() API might query on two machines sharded by user Id. From the sender and recieverId we resolve the hashes of the machines and process on them. 

```java
sendMessage(senderId, receiverId, message) {
    senderShard = getShard(senderId);
    receiverShard = getShard(receiverId);

    senderDB = getDB(senderShard);
    receiverDB = getDB(receiverShard);

    senderDB.storeMessage(message);
    receiverDB.storeMessage(message);
}
```

These processes on DB1(machine where sender messages are present) and DB2(machine where receiver messages are present) can fail.

**DB1 success, DB2 success**. No data inconsistency. This is fine.
**DB1 fails, DB2 fails**. No data inconsistency. 
**DB1 success, DB2 fails**. Data inconsistency. Sender finds the data as sent. Receiver will not.
**DB1 fails, DB2 success**. (Here we write to DB2 first before DB1). Sender might find it as not sent, but receiver has received it. Sender is already aware of the data, so even if they see it as not sent, its less problematic than sender believing the data was received while receiver not.

___
- **Choosing the DB**
We know in FB Messenger the read-write is 1:1 i.e. both read and write heavy. So we need to reduce one of them. 

Lets try to reduce writes first. **In google typeahead, we reduced writes by batching it and only letting 1% of the writes reach the database.** The same approach is not applicable here as all user messages need to reach the database, as consistency is very very important here. **Therefore we cannot reduce write.**

Lets try to reduce reads. Lets say in this design the database is only taking care of writes, while all the reads are being taken care to somewhere else called **X**. **X** should be consistent with the database, and therefore any write that comes to DB should go to **X** as well. 

**X should be**
- consistent with the DB
- able to handle a large number of writes
- able to handle a large number of reads

=> X should not store on disk as it needs large number of writes and reads. 
=> X should store data in RAM
=> X should be a cache

The main database should be there to handle a large number of writes while we have a cache to handle the above.

**What kind of cache is very consistent with the database?**
a Write-Through cache

The application server will first write to the cache, and then to the database. Any read query will only read from the cache. Here cache size is small and will not be able to store all the chats, but in a messenger, we rarely look at old messages.

**Further optimization**
With this design there are latency between application server and cache. 
- Writing latency
14. Application server and cache
15. Application server and DB
=> Slow system

- Read latency
16. Application server and cache

We can reduce this latency by implementing a local cache within the application server. This'd make the application server stateful. If there are N application server, there will be N cache and if load balancer had forwarded the requests randomly or by round robin approach, some requests of a user would be in one app server and other in another. Ideally we want all the data of a user in one app server. 

>So the load balancer need to forward the requests based on user id as well so that all data of a user will be in one application server.

![[Screenshot 2025-02-01 141654.png]]
When a request gets to load balancer, it forwards the request based on the userId. This application sorts out the application server of the receiver first and sends the request to that server through HTTP. This second server will write to its local cache, then communicate with the shard of the receiver. After that the first application server writes to its own cache and communicates with the shard of the sender. (First receiver and then sender).

Now finally, what DB should we use?
Any DB which can handle a large number of writes, like Cassandra. We set the **W** as low leading to faster writes.

NOTE: We do not use document DB as we don't have a unstructured data here. Key value will also not word as the value has multiple parameters.

#### 3. Bloom Filters
- Whenever you have to search for anything, we go to the database
- To check if something exists or not, we go to the database.
Both of these involves latency and some time database take to execute a query.

In case of NoSQL DB, they internally use LSL Tree Datastructure to execute a query. 

Lets say we got a find(k) query
- First the MemTable will be checked. If its not present there,
- then the SSTables(files) will be checked from latest to oldest.
If still not present => the data doesn't exists. 
Therefore **searching a key that doesn't exist will be the most time consuming**. So if we know whether a key exist beforehand, then we can optimize the system.

This is where bloom filters come into play.

**Bloom filters**
When input with a key, bloom filter will return true or false. 
true => The key **may or may not exist.**
false => The key 100% do not exist.

So a good bloom filter is one which has a low false positives.

**How bloom filters work?**
Blook filter has
- a bit array. Bigger the bit array, lower the chance of false positives. A bit array of 16 bits has a size of 2 bytes. 
- multiple hash functions. More hash functions mean lower the chance of false positives. 

Each hash function will return from 0 to n-1, where n is the size of he bit array. Whenever someone inserts a key, you first compute the return value (0 to n-1) of that key from all hash functions. 

Lets say we have a 16 bit array and 3 hash functions h1, h2, h3.

inserts(key1)

h1(key1) returns 4
h2(key1) returns 7
h3(key1) returns 13

We set the bit on all the positions returned from the hash functions
updated bit array => 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0

inserts(key2)

h1(key2) returns 8
h2(key2) returns 7
h3(key2) returns 11

updated bit array => 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0

Now, lets say someone searches for a key
**search(key3)**

h1(key3) returns 0
h2(key3) returns 8
h3(key3) returns 11

We check if the above bits are set in the bit array.
**The key is only present if all the bits are set for the given search key.**

Now lets take a false positive scenario with a non existent key, key4. 

h1(key4) returns 4
h2(key4) returns 7
h3(key4) returns 11

Here all the bits are set, but key4 is not present. The bigger the array and more the hash functions, the probability of this happening will reduce. 

NOTE: Obviously we do not unset the bit when we delete a key. During delete we don't do anything.

```java
get(key){
	boolean exists = bf.exists(key);
	if(!exists){
		return -1;
	}
	else{
		/*carry out the search from LSM Table
		.
		.
		.
		*/
	}
}
```

___
